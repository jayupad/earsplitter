{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJnJK1aCts1E"
      },
      "source": [
        "# Create data directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install torchaudio librosa soundfile numpy matplotlib IPython"
      ],
      "metadata": {
        "id": "9DN7MHjAwZX5",
        "outputId": "8ed75caa-8839-4431-b768-3c85d5083168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rdRzVOzgECI",
        "outputId": "35394b84-5a38-4ac5-f7b5-2f0721d6e420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.95G/5.95G [03:02<00:00, 34.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchaudio.datasets.librispeech.LIBRISPEECH at 0x78f4423d62d0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lUkVRKZtz7X"
      },
      "source": [
        "# Download LibriSpeech (train-clean-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJfq3sZ7s2qf",
        "outputId": "fb90c6a7-6322-48e2-91ec-afa0aecd2bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample rate: 16000\n",
            "Transcript: CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK\n",
            "Waveform shape: torch.Size([1, 225360])\n"
          ]
        }
      ],
      "source": [
        "dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "\n",
        "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = dataset[0]\n",
        "\n",
        "print(f\"Sample rate: {sample_rate}\")\n",
        "print(f\"Transcript: {transcript}\")\n",
        "print(f\"Waveform shape: {waveform.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0K1Cud8t-0F"
      },
      "source": [
        "# Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WY5kvd5qs44c"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import json\n",
        "\n",
        "def clean_data(min_duration=1.5, max_peak=0.95, sr=16000, save_dir=\"spectrogram_data\"):\n",
        "    dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "    # mel_transform = T.MelSpectrogram(sample_rate=sr, n_fft=512, hop_length=128, n_mels=64)\n",
        "\n",
        "    cleaned = []\n",
        "\n",
        "    if os.path.exists('meta/clean_metadata.json'):\n",
        "        with open('meta/clean_metadata.json', 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(\"meta\", exist_ok=True)\n",
        "\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        waveform, sample_rate, _, speaker_id, _, utt_id = dataset[i]\n",
        "\n",
        "        duration = waveform.shape[1] / sample_rate\n",
        "        peak_val = waveform.abs().max().item()\n",
        "\n",
        "        if duration >= min_duration and peak_val <= max_peak:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "            # Convert to spectrogram\n",
        "            # spec = mel_transform(waveform)\n",
        "\n",
        "            # filename = f\"{speaker_id}_{utt_id}.pt\"\n",
        "            # path = os.path.join(save_dir, filename)\n",
        "            # torch.save(spec, path)\n",
        "\n",
        "            cleaned.append({\n",
        "                \"index\": i,\n",
        "                \"duration\": duration,\n",
        "                \"speaker_id\": int(speaker_id),\n",
        "                \"utterance_id\": int(utt_id)\n",
        "                # \"spectrogram_path\": path\n",
        "            })\n",
        "\n",
        "\n",
        "    with open(\"meta/clean_metadata.json\", \"w\") as f:\n",
        "        json.dump(cleaned, f, indent=2)\n",
        "\n",
        "    # print(f\"✅ Converted and saved {len(cleaned)} spectrograms to '{save_dir}'\")\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ef8cAl5nwE4J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def create_data(num_samples, duration=5.0, seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Load dataset (no download here since we already have it)\n",
        "    dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "    with open(\"meta/clean_metadata.json\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    mel_transform = T.MelSpectrogram(\n",
        "        sample_rate=16000,\n",
        "        n_fft=1024,\n",
        "        hop_length=256,\n",
        "        n_mels=64\n",
        "    )\n",
        "\n",
        "    examples = []\n",
        "    for _ in range(num_samples):\n",
        "        while True:\n",
        "            s1, s2 = random.sample(range(len(metadata)), 2)\n",
        "            meta1, meta2 = metadata[s1], metadata[s2]\n",
        "            if meta1[\"speaker_id\"] != meta2[\"speaker_id\"]:\n",
        "                break\n",
        "\n",
        "        wave1, sr, *_ = dataset[meta1[\"index\"]]\n",
        "        wave2, sr, *_ = dataset[meta2[\"index\"]]\n",
        "\n",
        "        max_len = int(duration * sr)\n",
        "        wave1 = wave1[:, :max_len]\n",
        "        wave2 = wave2[:, :max_len]\n",
        "\n",
        "        # Pad if needed (ensure equal length)\n",
        "        def pad(wave, target_len):\n",
        "            if wave.shape[1] < target_len:\n",
        "                pad_amt = target_len - wave.shape[1]\n",
        "                wave = torch.nn.functional.pad(wave, (0, pad_amt))\n",
        "            return wave\n",
        "\n",
        "        wave1 = pad(wave1, max_len)\n",
        "        wave2 = pad(wave2, max_len)\n",
        "\n",
        "        # Mix the two: wave1 is target, wave2 is background\n",
        "        mix = 0.8 * wave1 + 0.2 * wave2\n",
        "\n",
        "        mix_spec = mel_transform(mix).squeeze(0)      # shape: [n_mels, T']\n",
        "        target_spec = mel_transform(wave1).squeeze(0)  # shape: [n_mels, T']\n",
        "\n",
        "        examples.append((mix_spec, target_spec))\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b16EuK9tLY1U"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class SplitterRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "    super(SplitterRNN, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size=input_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        num_layers=num_layers,\n",
        "                        batch_first=True,\n",
        "                        dropout=dropout)\n",
        "    self.linear = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.transpose(1, 2)\n",
        "    output, _ = self.lstm(x)  # output: (batch_size, time_steps, hidden_size)\n",
        "    output = self.linear(output)  # output: (batch_size, time_steps, input_size)\n",
        "    return output.transpose(1,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZuoYMMx6wE4K"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        self.examples = examples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "def collate_mel(batch):\n",
        "    mix_specs, target_specs = zip(*batch)  # list of [n_mels, T_i]\n",
        "\n",
        "    mix_padded = pad_sequence(mix_specs, batch_first=True)       # [B, T_max, n_mels]\n",
        "    target_padded = pad_sequence(target_specs, batch_first=True) # [B, T_max, n_mels]\n",
        "\n",
        "    # transpose to [B, n_mels, T] if your model expects that\n",
        "    mix_padded = mix_padded.transpose(1, 2)       # [B, n_mels, T]\n",
        "    target_padded = target_padded.transpose(1, 2) # [B, n_mels, T]\n",
        "\n",
        "    return mix_padded, target_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ylQfmpik50Ed"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def simple_collate(batch):\n",
        "    mix_specs, target_specs = zip(*batch)\n",
        "    mix_batch = torch.stack(mix_specs)       # [B, n_mels, T]\n",
        "    target_batch = torch.stack(target_specs) # [B, n_mels, T]\n",
        "    return mix_batch.to(device=device), target_batch.to(device=device)\n",
        "\n",
        "clean_data(min_duration=5.0)\n",
        "constructed_dataset = create_data(10000, 5.0)\n",
        "split = int(0.8 * len(constructed_dataset))\n",
        "train_examples = constructed_dataset[:split]\n",
        "test_examples = constructed_dataset[split:]\n",
        "\n",
        "train_dataset = SpectrogramDataset(train_examples)\n",
        "test_dataset = SpectrogramDataset(test_examples)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=simple_collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=simple_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T4SvcNkzNk-U"
      },
      "outputs": [],
      "source": [
        "def train_rnn(model, criterion, optimizer, train_loader, nepochs=5000):\n",
        "    losses = []\n",
        "    model.train()\n",
        "    for e in range(nepochs):\n",
        "      total_loss = 0.0\n",
        "      for mixes_batch, tgt_batch in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          output = model(mixes_batch)\n",
        "          loss = criterion(output, tgt_batch)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      losses.append(avg_loss)\n",
        "      if e % 5 == 0:\n",
        "        print(f\"Epoch {e:2d} | Loss: {avg_loss:.6f}\")\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FxnR-Az-wE4K",
        "outputId": "d118eae0-8602-4acb-f792-0d6f60872433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0 | Loss: 10380.914496\n",
            "Epoch  5 | Loss: 8539.529343\n",
            "Epoch 10 | Loss: 7461.977280\n",
            "Epoch 15 | Loss: 6613.057019\n",
            "Epoch 20 | Loss: 5929.655035\n",
            "Epoch 25 | Loss: 5369.437668\n",
            "Epoch 30 | Loss: 4917.615665\n",
            "Epoch 35 | Loss: 4551.314039\n",
            "Epoch 40 | Loss: 4239.506692\n",
            "Epoch 45 | Loss: 3976.131297\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10380.914496459962,\n",
              " 9754.39778149414,\n",
              " 9393.959757202148,\n",
              " 9084.584596069337,\n",
              " 8800.050200622558,\n",
              " 8539.529343383789,\n",
              " 8296.778997528076,\n",
              " 8068.640504882813,\n",
              " 7855.19833190918,\n",
              " 7653.193322814941,\n",
              " 7461.977279907226,\n",
              " 7277.634616394043,\n",
              " 7099.590154968262,\n",
              " 6929.778098388671,\n",
              " 6768.10541015625,\n",
              " 6613.057018981934,\n",
              " 6464.515320983886,\n",
              " 6322.100944702149,\n",
              " 6184.706126159668,\n",
              " 6053.892711395264,\n",
              " 5929.655035217285,\n",
              " 5808.644591979981,\n",
              " 5693.68315826416,\n",
              " 5583.822913848877,\n",
              " 5475.264801391601,\n",
              " 5369.437667541504,\n",
              " 5269.90053704834,\n",
              " 5175.854326293946,\n",
              " 5086.006725830078,\n",
              " 4999.3899889526365,\n",
              " 4917.615664611816,\n",
              " 4839.3344080200195,\n",
              " 4763.3142830200195,\n",
              " 4690.814848693848,\n",
              " 4619.038354980469,\n",
              " 4551.314039428711,\n",
              " 4483.229152160645,\n",
              " 4420.014795959472,\n",
              " 4356.944166290284,\n",
              " 4298.10276159668,\n",
              " 4239.506692047119,\n",
              " 4184.349056182861,\n",
              " 4130.973856048584,\n",
              " 4079.058049606323,\n",
              " 4027.055926269531,\n",
              " 3976.1312973327636,\n",
              " 3926.381951828003,\n",
              " 3883.24719039917,\n",
              " 3835.8638897247315,\n",
              " 3791.1066659240723]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Parameters\n",
        "input_size = 64\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "batch_size = 8\n",
        "sequence_length = 100\n",
        "dropout = 0.25\n",
        "lr = 0.001\n",
        "\n",
        "model = SplitterRNN(input_size, hidden_size, num_layers, dropout).to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
        "\n",
        "train_rnn(model, criterion, optimizer, train_loader, nepochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RzHq_vKjNmBf"
      },
      "outputs": [],
      "source": [
        "def test_rnn(model, criterion, test_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mixes_batch, tgt_batch in test_loader:\n",
        "            output = model(mixes_batch)\n",
        "            loss = criterion(output, tgt_batch)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_loss:.6f}\")\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = test_rnn(model, criterion, test_loader)\n",
        "test_loss"
      ],
      "metadata": {
        "id": "1HlVrwNQ5Yud",
        "outputId": "f45c4cb6-c6b4-40a7-8699-cd2e2db24df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 3743.773630\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3743.773629760742"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO8-9FPJJZXT"
      },
      "source": [
        "# Designing Computational Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNgtAhTOK8xv"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dziZFKyfM2wB"
      },
      "outputs": [],
      "source": [
        "pip install scipy sounddevice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "pj8qqM6sK8SH",
        "outputId": "7ac733df-0dbb-4426-ab33-549f261c49ab"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sounddevice'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7a97a6e9da78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# play_wav_file(\"path/to/your/audio.wav\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "\n",
        "# play_wav_file(\"path/to/your/audio.wav\")\n",
        "def play_audio(audio, sample_rate=44100):\n",
        "    \"\"\"\n",
        "    Plays an audio sample.\n",
        "\n",
        "    Parameters:\n",
        "    - audio (numpy.ndarray): The audio data. Can be mono (1D) or stereo (2D).\n",
        "    - sample_rate (int): Sampling rate in Hz (default: 44100).\n",
        "    \"\"\"\n",
        "    if not isinstance(audio, np.ndarray):\n",
        "        raise ValueError(\"Audio must be a NumPy array.\")\n",
        "\n",
        "    if audio.ndim > 2 or (audio.ndim == 2 and audio.shape[1] > 2):\n",
        "        raise ValueError(\"Audio must be mono or stereo.\")\n",
        "\n",
        "    sd.play(audio, samplerate=sample_rate)\n",
        "    sd.wait()  # Wait until audio playback is finished\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# visualize_wav(\"path/to/your/audio.wav\")\n",
        "def visualize_wav(filepath, show_spectrogram=True):\n",
        "    \"\"\"\n",
        "    Visualizes the waveform (and optionally spectrogram) of a WAV file.\n",
        "\n",
        "    Parameters:\n",
        "    - filepath (str): Path to the WAV file.\n",
        "    - show_spectrogram (bool): Whether to show the spectrogram below the waveform.\n",
        "    \"\"\"\n",
        "    # Load audio\n",
        "    data, samplerate = sf.read(filepath)\n",
        "    duration = len(data) / samplerate\n",
        "    time = np.linspace(0, duration, num=len(data))\n",
        "\n",
        "    # Set up plot\n",
        "    fig, axs = plt.subplots(2 if show_spectrogram else 1, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "    if not isinstance(axs, np.ndarray):\n",
        "        axs = [axs]  # ensure axs is always a list for consistency\n",
        "\n",
        "    # Plot waveform\n",
        "    axs[0].plot(time, data)\n",
        "    axs[0].set_title('Waveform')\n",
        "    axs[0].set_ylabel('Amplitude')\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Optional: Plot spectrogram\n",
        "    if show_spectrogram:\n",
        "        axs[1].specgram(data[:, 0] if data.ndim > 1 else data, Fs=samplerate, NFFT=1024, noverlap=512)\n",
        "        axs[1].set_title('Spectrogram')\n",
        "        axs[1].set_ylabel('Frequency [Hz]')\n",
        "        axs[1].set_xlabel('Time [s]')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Example:\n",
        "\n",
        "# Load audio files\n",
        "speaker1, sr = librosa.load(\"speaker1.wav\", sr=None)\n",
        "speaker2, _ = librosa.load(\"speaker2.wav\", sr=None)\n",
        "\n",
        "# Mix with speaker2 at 4x the volume of speaker1\n",
        "mixed, s1_adj, s2_adj = adjust_loudness_mix(speaker1, speaker2, loudness_ratio=4.0)\n",
        "\n",
        "# Save to disk\n",
        "librosa.output.write_wav(\"mixed.wav\", mixed, sr)\n",
        "\"\"\"\n",
        "def adjust_loudness_mix(speaker1_wave, speaker2_wave, loudness_ratio):\n",
        "    \"\"\"\n",
        "    Adjusts the loudness of speaker2 relative to speaker1.\n",
        "\n",
        "    Parameters:\n",
        "        speaker1_wave (np.ndarray): Audio waveform of speaker 1.\n",
        "        speaker2_wave (np.ndarray): Audio waveform of speaker 2.\n",
        "        loudness_ratio (float): Factor to scale speaker2's amplitude.\n",
        "                                 e.g., 2.0 means speaker2 is twice as loud as speaker1.\n",
        "\n",
        "    Returns:\n",
        "        mixed_wave (np.ndarray): The resulting mixture of the two speakers.\n",
        "        adjusted_speaker2 (np.ndarray): The scaled version of speaker2_wave.\n",
        "    \"\"\"\n",
        "    # Match length if needed\n",
        "    min_len = min(len(speaker1_wave), len(speaker2_wave))\n",
        "    speaker1_wave = speaker1_wave[:min_len]\n",
        "    speaker2_wave = speaker2_wave[:min_len]\n",
        "\n",
        "    # Apply loudness adjustment to speaker2\n",
        "    adjusted_speaker2 = speaker2_wave * loudness_ratio\n",
        "\n",
        "    # Mix the two signals\n",
        "    mixed_wave = speaker1_wave + adjusted_speaker2\n",
        "\n",
        "    # Optional: Normalize to avoid clipping\n",
        "    max_val = np.max(np.abs(mixed_wave))\n",
        "    if max_val > 1.0:\n",
        "        mixed_wave = mixed_wave / max_val\n",
        "        adjusted_speaker2 = adjusted_speaker2 / max_val\n",
        "        speaker1_wave = speaker1_wave / max_val\n",
        "\n",
        "    return mixed_wave, speaker1_wave, adjusted_speaker2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUpxVuhtCocL"
      },
      "source": [
        "# Nate does some delicious testing here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "VKfAA4XLDh7A",
        "outputId": "c228467d-09ae-43c7-faea-184480b3266b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'play_audio' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b81ebf930c8b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutterance_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'play_audio' is not defined"
          ]
        }
      ],
      "source": [
        "# dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = dataset[0]\n",
        "\n",
        "play_audio(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8V-Z5kFKtOL"
      },
      "source": [
        "**Experiment 1 - adjust loudness of target input and compare loss**\n",
        "\n",
        "Goal: Assess how robust the model is when one speaker is significantly louder.\n",
        "\n",
        "Design:\n",
        "\n",
        "For each mixed sample, systematically vary the amplitude of one speaker:\n",
        "\n",
        " - Ratios: 1:1 (baseline), 2:1, 4:1, 8:1\n",
        "\n",
        "Evaluate performance degradation or adaptation.\n",
        "\n",
        "Hypotheses:\n",
        "\n",
        "MSE may degrade linearly with imbalance.\n",
        "\n",
        "CTC might maintain higher robustness if sequences are still distinguishable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STJpcYsAJUNf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQtlGyf8K138"
      },
      "source": [
        "Experiment 2 - adjust loudness of background noise and compare loss (I think this may combine with experiments 1 and 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYgarPFfK2Sx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIZx4O-OK2w-"
      },
      "source": [
        "**Experiment 3 - compare efficiency on different languages**\n",
        "\n",
        "Goal: Evaluate performance when one or both speakers speak a non-English language.\n",
        "\n",
        "Design:\n",
        "\n",
        "Use mixtures of English-English, English-Other (e.g., English-Spanish), and Other-Other.\n",
        "\n",
        "Focus on tonal vs non-tonal languages for diversity.\n",
        "\n",
        "Optional:\n",
        "\n",
        "Augment training data with multilingual samples.\n",
        "\n",
        "Try language embeddings if you're using a deeper pipeline.\n",
        "\n",
        "Analysis:\n",
        "\n",
        "Does the model performance degrade with unfamiliar phonetic structures?\n",
        "\n",
        "Does it favor English content due to training bias?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTvWI7qYK3GL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAfLe9pfL6km"
      },
      "source": [
        "**Experiment 4 - add non-human background noise and compare loss**\n",
        "\n",
        "Goal: Evaluate robustness to noise interference (e.g., cafe noise, car engines, music).\n",
        "\n",
        "Design:\n",
        "\n",
        "Add non-human noise at varying SNRs (e.g., 30 dB, 20 dB, 10 dB).\n",
        "\n",
        "Use both stationary and non-stationary noise.\n",
        "\n",
        "Compare:\n",
        "\n",
        "Performance with and without noise-aware preprocessing.\n",
        "\n",
        "Try noise suppression front-ends or data augmentation with noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm9HcraeL63U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPZ68Ohr4hVB"
      },
      "source": [
        "**Extra Experiments You Could Add**\n",
        "\n",
        "Speaker permutation invariance: Does the model consistently output the same speaker when order varies?\n",
        "\n",
        "Speaker gender and pitch: Investigate separation accuracy across gender combinations or pitch similarity.\n",
        "\n",
        "Temporal length variation: Does it work on long conversations vs short utterances?\n",
        "\n",
        "Window size sensitivity: Test how performance changes with different input chunk sizes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}