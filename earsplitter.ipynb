{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myV7aFcptTcb"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_kIVGEyVZ7v",
        "outputId": "6cbea410-ec35-42ad-b1d3-b4f340f5b623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install torchaudio librosa soundfile numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJnJK1aCts1E"
      },
      "source": [
        "# Create data directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgQGhFxsXjmr",
        "outputId": "7b11e37e-9f7b-4a24-ab7d-7850a191472e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython) (75.2.0)\n",
            "Collecting jedi>=0.16 (from IPython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "pip install matplotlib IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BZCC-Vv7spuh"
      },
      "outputs": [],
      "source": [
        "mkdir -p ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rdRzVOzgECI",
        "outputId": "8b527d64-3aeb-4c97-f250-4ccec49ecb42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torchaudio.datasets.librispeech.LIBRISPEECH at 0x105a213d0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lUkVRKZtz7X"
      },
      "source": [
        "# Download LibriSpeech (train-clean-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJfq3sZ7s2qf",
        "outputId": "0b1e6701-4fb0-4354-b5d7-e0bfa43aa8df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample rate: 16000\n",
            "Transcript: CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK\n",
            "Waveform shape: torch.Size([1, 225360])\n"
          ]
        }
      ],
      "source": [
        "dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "\n",
        "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = dataset[0]\n",
        "\n",
        "print(f\"Sample rate: {sample_rate}\")\n",
        "print(f\"Transcript: {transcript}\")\n",
        "print(f\"Waveform shape: {waveform.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0K1Cud8t-0F"
      },
      "source": [
        "# Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WY5kvd5qs44c"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "\n",
        "def clean_data(min_duration=1.5, max_peak=0.95, sr=16000, save_dir=\"spectrogram_data\"):\n",
        "    dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "    mel_transform = T.MelSpectrogram(sample_rate=sr, n_fft=512, hop_length=128, n_mels=64)\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(\"meta\", exist_ok=True)\n",
        "\n",
        "    cleaned = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        waveform, sample_rate, _, speaker_id, _, utt_id = dataset[i]\n",
        "\n",
        "        duration = waveform.shape[1] / sample_rate\n",
        "        peak_val = waveform.abs().max().item()\n",
        "\n",
        "        if duration >= min_duration and peak_val <= max_peak:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "            # Convert to spectrogram\n",
        "            spec = mel_transform(waveform)\n",
        "\n",
        "            filename = f\"{speaker_id}_{utt_id}.pt\"\n",
        "            path = os.path.join(save_dir, filename)\n",
        "            torch.save(spec, path)\n",
        "\n",
        "            cleaned.append({\n",
        "                \"index\": i,\n",
        "                \"duration\": duration,\n",
        "                \"speaker_id\": int(speaker_id),\n",
        "                \"utterance_id\": int(utt_id),\n",
        "                \"spectrogram_path\": path\n",
        "            })\n",
        "\n",
        "    with open(\"meta/clean_metadata.json\", \"w\") as f:\n",
        "        json.dump(cleaned, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Converted and saved {len(cleaned)} spectrograms to '{save_dir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vIY_qhJ31wRW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "\n",
        "def create_train_data(configs, output_dir=\"train_data\", seed=42):\n",
        "    # Set random seeds so we can reproduce results\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Load cleaned metadata\n",
        "    with open(\"meta/clean_metadata.json\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # Load dataset (no download here since we already have it)\n",
        "    dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(configs[\"num_samples\"]):\n",
        "        if configs[\"type\"] == \"cocktail\":\n",
        "            # Pick multiple random speakers\n",
        "            speakers = random.sample(metadata, configs[\"num_speakers\"])\n",
        "            waves = []\n",
        "\n",
        "            # Figure out the shortest clip (so we can align them all)\n",
        "            shortest_len = None\n",
        "            for s in speakers:\n",
        "                wave, sr, *_ = dataset[s[\"index\"]]\n",
        "                wave = wave[:, :int(sr * configs[\"duration\"])]\n",
        "                if shortest_len is None or wave.shape[1] < shortest_len:\n",
        "                    shortest_len = wave.shape[1]\n",
        "\n",
        "            # Truncate everything to match the shortest, apply random volume\n",
        "            for s in speakers:\n",
        "                wave, sr, *_ = dataset[s[\"index\"]]\n",
        "                wave = wave[:, :shortest_len]\n",
        "                amp = random.uniform(0.01, 0.1)  # really soft background voices\n",
        "                waves.append(amp * wave)\n",
        "\n",
        "            # Sum up all the quiet speakers — one louder target is at index 0\n",
        "            mix = sum(waves)\n",
        "            target = waves[0]  # target speaker is just the first one we picked\n",
        "\n",
        "        elif configs[\"type\"] == \"twospeaker\":\n",
        "            # Just pick two people to mix\n",
        "            s1, s2 = random.sample(metadata, 2)\n",
        "            wave1, sr, *_ = dataset[s1[\"index\"]]\n",
        "            wave2, sr, *_ = dataset[s2[\"index\"]]\n",
        "\n",
        "            # Truncate to match lengths\n",
        "            min_len = min(wave1.shape[1], wave2.shape[1])\n",
        "            wave1 = wave1[:, :min_len]\n",
        "            wave2 = wave2[:, :min_len]\n",
        "\n",
        "            # Speaker 1 is loud, speaker 2 is background\n",
        "            mix = 0.8 * wave1 + 0.2 * wave2\n",
        "            target = wave1\n",
        "\n",
        "        sf.write(f\"{output_dir}/mix_{i}.wav\", mix.squeeze().numpy(), sr)\n",
        "        sf.write(f\"{output_dir}/target_{i}.wav\", target.squeeze().numpy(), sr)\n",
        "\n",
        "    print(f\"✔️ Done! Made {configs['num_samples']} samples in '{output_dir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylQfmpik50Ed",
        "outputId": "43a34d01-4e56-40cd-9033-be61a2270ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Converted and saved 27840 spectrograms to 'spectrogram_data'\n",
            "✔️ Done! Made 100 samples in 'train_data'\n",
            "✔️ Done! Made 100 samples in 'train_data'\n"
          ]
        }
      ],
      "source": [
        "cocktail_config = {\n",
        "    \"type\": \"cocktail\",\n",
        "    \"num_samples\": 100,\n",
        "    \"num_speakers\": 15,\n",
        "    \"duration\": 3.0\n",
        "}\n",
        "\n",
        "twospeaker_config = {\n",
        "    \"type\": \"twospeaker\",\n",
        "    \"num_samples\": 100,\n",
        "}\n",
        "clean_data()\n",
        "create_train_data(cocktail_config)\n",
        "create_train_data(twospeaker_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b16EuK9tLY1U"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class SplitterRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.3):\n",
        "    super(SplitterRNN, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size=input_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        num_layers=num_layers,\n",
        "                        batch_first=True,\n",
        "                        dropout=dropout)\n",
        "    self.linear = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output, _ = self.lstm(x)  # output: (batch_size, time_steps, hidden_size)\n",
        "    output = self.linear(output)  # output: (batch_size, time_steps, input_size)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0go3dF9NytZ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Parameters\n",
        "input_size = 257\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "batch_size = 32\n",
        "sequence_length = 100\n",
        "dropout = 0.25\n",
        "lr = 0.001\n",
        "\n",
        "model = SplitterRNN(input_size, hidden_size, num_layers, dropout).to(device=device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGbCgeG5Tpj_",
        "outputId": "623af01a-ee26-4fda-df6a-5096f09bdc42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch mix shape: torch.Size([8, 1, 48000])\n",
            "Batch target shape: torch.Size([8, 1, 48000])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchaudio\n",
        "import os\n",
        "\n",
        "class SpeechSeparationDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        files = os.listdir(self.data_dir)\n",
        "        wav_files = [f for f in files if f.endswith(\".wav\")]\n",
        "        ids = {f.split(\"_\")[1].split(\".\")[0] for f in wav_files}\n",
        "        self.sample_ids = sorted(ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.sample_ids[idx]\n",
        "        mix_path = os.path.join(self.data_dir, f\"mix_{sample_id}.wav\")\n",
        "        target_path = os.path.join(self.data_dir, f\"target_{sample_id}.wav\")\n",
        "\n",
        "        mix, _ = torchaudio.load(mix_path)\n",
        "        target, _ = torchaudio.load(target_path)\n",
        "\n",
        "        return mix, target\n",
        "\n",
        "# TODO: Computational Experiment (pad or truncate??)\n",
        "def pad_inputs(batch):\n",
        "    mixed_audio, target_audio = zip(*batch)\n",
        "    mixed_audio = [x.squeeze(0) for x in mixed_audio]\n",
        "    target_audio = [x.squeeze(0) for x in target_audio]\n",
        "\n",
        "    mixes = pad_sequence(mixed_audio, batch_first=True)\n",
        "    targets = pad_sequence(target_audio, batch_first=True)\n",
        "\n",
        "    return mixes.unsqueeze(1), targets.unsqueeze(1)\n",
        "\n",
        "train_dataset = SpeechSeparationDataset(\"train_data\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pad_inputs)\n",
        "\n",
        "# Iterate through batches\n",
        "for mix_batch, target_batch in train_loader:\n",
        "    print(\"Batch mix shape:\", mix_batch.shape)\n",
        "    print(\"Batch target shape:\", target_batch.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4SvcNkzNk-U"
      },
      "outputs": [],
      "source": [
        "def train_rnn(model, criterion, optimizer, delay, delay_range=None, verbose=True, nepochs=5000):\n",
        "  losses = []\n",
        "  for e in nepochs:\n",
        "\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzHq_vKjNmBf"
      },
      "outputs": [],
      "source": [
        "def test_rnn():\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO8-9FPJJZXT"
      },
      "source": [
        "# Designing Computational Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNgtAhTOK8xv"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dziZFKyfM2wB"
      },
      "outputs": [],
      "source": [
        "pip install scipy sounddevice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "pj8qqM6sK8SH",
        "outputId": "7ac733df-0dbb-4426-ab33-549f261c49ab"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sounddevice'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7a97a6e9da78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# play_wav_file(\"path/to/your/audio.wav\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "\n",
        "# play_wav_file(\"path/to/your/audio.wav\")\n",
        "def play_audio(audio, sample_rate=44100):\n",
        "    \"\"\"\n",
        "    Plays an audio sample.\n",
        "\n",
        "    Parameters:\n",
        "    - audio (numpy.ndarray): The audio data. Can be mono (1D) or stereo (2D).\n",
        "    - sample_rate (int): Sampling rate in Hz (default: 44100).\n",
        "    \"\"\"\n",
        "    if not isinstance(audio, np.ndarray):\n",
        "        raise ValueError(\"Audio must be a NumPy array.\")\n",
        "\n",
        "    if audio.ndim > 2 or (audio.ndim == 2 and audio.shape[1] > 2):\n",
        "        raise ValueError(\"Audio must be mono or stereo.\")\n",
        "\n",
        "    sd.play(audio, samplerate=sample_rate)\n",
        "    sd.wait()  # Wait until audio playback is finished\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# visualize_wav(\"path/to/your/audio.wav\")\n",
        "def visualize_wav(filepath, show_spectrogram=True):\n",
        "    \"\"\"\n",
        "    Visualizes the waveform (and optionally spectrogram) of a WAV file.\n",
        "\n",
        "    Parameters:\n",
        "    - filepath (str): Path to the WAV file.\n",
        "    - show_spectrogram (bool): Whether to show the spectrogram below the waveform.\n",
        "    \"\"\"\n",
        "    # Load audio\n",
        "    data, samplerate = sf.read(filepath)\n",
        "    duration = len(data) / samplerate\n",
        "    time = np.linspace(0, duration, num=len(data))\n",
        "\n",
        "    # Set up plot\n",
        "    fig, axs = plt.subplots(2 if show_spectrogram else 1, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "    if not isinstance(axs, np.ndarray):\n",
        "        axs = [axs]  # ensure axs is always a list for consistency\n",
        "\n",
        "    # Plot waveform\n",
        "    axs[0].plot(time, data)\n",
        "    axs[0].set_title('Waveform')\n",
        "    axs[0].set_ylabel('Amplitude')\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Optional: Plot spectrogram\n",
        "    if show_spectrogram:\n",
        "        axs[1].specgram(data[:, 0] if data.ndim > 1 else data, Fs=samplerate, NFFT=1024, noverlap=512)\n",
        "        axs[1].set_title('Spectrogram')\n",
        "        axs[1].set_ylabel('Frequency [Hz]')\n",
        "        axs[1].set_xlabel('Time [s]')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Example:\n",
        "\n",
        "# Load audio files\n",
        "speaker1, sr = librosa.load(\"speaker1.wav\", sr=None)\n",
        "speaker2, _ = librosa.load(\"speaker2.wav\", sr=None)\n",
        "\n",
        "# Mix with speaker2 at 4x the volume of speaker1\n",
        "mixed, s1_adj, s2_adj = adjust_loudness_mix(speaker1, speaker2, loudness_ratio=4.0)\n",
        "\n",
        "# Save to disk\n",
        "librosa.output.write_wav(\"mixed.wav\", mixed, sr)\n",
        "\"\"\"\n",
        "def adjust_loudness_mix(speaker1_wave, speaker2_wave, loudness_ratio):\n",
        "    \"\"\"\n",
        "    Adjusts the loudness of speaker2 relative to speaker1.\n",
        "\n",
        "    Parameters:\n",
        "        speaker1_wave (np.ndarray): Audio waveform of speaker 1.\n",
        "        speaker2_wave (np.ndarray): Audio waveform of speaker 2.\n",
        "        loudness_ratio (float): Factor to scale speaker2's amplitude.\n",
        "                                 e.g., 2.0 means speaker2 is twice as loud as speaker1.\n",
        "\n",
        "    Returns:\n",
        "        mixed_wave (np.ndarray): The resulting mixture of the two speakers.\n",
        "        adjusted_speaker2 (np.ndarray): The scaled version of speaker2_wave.\n",
        "    \"\"\"\n",
        "    # Match length if needed\n",
        "    min_len = min(len(speaker1_wave), len(speaker2_wave))\n",
        "    speaker1_wave = speaker1_wave[:min_len]\n",
        "    speaker2_wave = speaker2_wave[:min_len]\n",
        "\n",
        "    # Apply loudness adjustment to speaker2\n",
        "    adjusted_speaker2 = speaker2_wave * loudness_ratio\n",
        "\n",
        "    # Mix the two signals\n",
        "    mixed_wave = speaker1_wave + adjusted_speaker2\n",
        "\n",
        "    # Optional: Normalize to avoid clipping\n",
        "    max_val = np.max(np.abs(mixed_wave))\n",
        "    if max_val > 1.0:\n",
        "        mixed_wave = mixed_wave / max_val\n",
        "        adjusted_speaker2 = adjusted_speaker2 / max_val\n",
        "        speaker1_wave = speaker1_wave / max_val\n",
        "\n",
        "    return mixed_wave, speaker1_wave, adjusted_speaker2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUpxVuhtCocL"
      },
      "source": [
        "# Nate does some delicious testing here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "VKfAA4XLDh7A",
        "outputId": "c228467d-09ae-43c7-faea-184480b3266b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'play_audio' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b81ebf930c8b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutterance_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'play_audio' is not defined"
          ]
        }
      ],
      "source": [
        "# dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = dataset[0]\n",
        "\n",
        "play_audio(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8V-Z5kFKtOL"
      },
      "source": [
        "**Experiment 1 - adjust loudness of target input and compare loss**\n",
        "\n",
        "Goal: Assess how robust the model is when one speaker is significantly louder.\n",
        "\n",
        "Design:\n",
        "\n",
        "For each mixed sample, systematically vary the amplitude of one speaker:\n",
        "\n",
        " - Ratios: 1:1 (baseline), 2:1, 4:1, 8:1\n",
        "\n",
        "Evaluate performance degradation or adaptation.\n",
        "\n",
        "Hypotheses:\n",
        "\n",
        "MSE may degrade linearly with imbalance.\n",
        "\n",
        "CTC might maintain higher robustness if sequences are still distinguishable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STJpcYsAJUNf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQtlGyf8K138"
      },
      "source": [
        "Experiment 2 - adjust loudness of background noise and compare loss (I think this may combine with experiments 1 and 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYgarPFfK2Sx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIZx4O-OK2w-"
      },
      "source": [
        "**Experiment 3 - compare efficiency on different languages**\n",
        "\n",
        "Goal: Evaluate performance when one or both speakers speak a non-English language.\n",
        "\n",
        "Design:\n",
        "\n",
        "Use mixtures of English-English, English-Other (e.g., English-Spanish), and Other-Other.\n",
        "\n",
        "Focus on tonal vs non-tonal languages for diversity.\n",
        "\n",
        "Optional:\n",
        "\n",
        "Augment training data with multilingual samples.\n",
        "\n",
        "Try language embeddings if you're using a deeper pipeline.\n",
        "\n",
        "Analysis:\n",
        "\n",
        "Does the model performance degrade with unfamiliar phonetic structures?\n",
        "\n",
        "Does it favor English content due to training bias?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTvWI7qYK3GL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAfLe9pfL6km"
      },
      "source": [
        "**Experiment 4 - add non-human background noise and compare loss**\n",
        "\n",
        "Goal: Evaluate robustness to noise interference (e.g., cafe noise, car engines, music).\n",
        "\n",
        "Design:\n",
        "\n",
        "Add non-human noise at varying SNRs (e.g., 30 dB, 20 dB, 10 dB).\n",
        "\n",
        "Use both stationary and non-stationary noise.\n",
        "\n",
        "Compare:\n",
        "\n",
        "Performance with and without noise-aware preprocessing.\n",
        "\n",
        "Try noise suppression front-ends or data augmentation with noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm9HcraeL63U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPZ68Ohr4hVB"
      },
      "source": [
        "**Extra Experiments You Could Add**\n",
        "\n",
        "Speaker permutation invariance: Does the model consistently output the same speaker when order varies?\n",
        "\n",
        "Speaker gender and pitch: Investigate separation accuracy across gender combinations or pitch similarity.\n",
        "\n",
        "Temporal length variation: Does it work on long conversations vs short utterances?\n",
        "\n",
        "Window size sensitivity: Test how performance changes with different input chunk sizes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "342venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
