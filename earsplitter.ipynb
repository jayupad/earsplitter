{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJnJK1aCts1E"
      },
      "source": [
        "# Create data directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rdRzVOzgECI",
        "outputId": "8b527d64-3aeb-4c97-f250-4ccec49ecb42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torchaudio.datasets.librispeech.LIBRISPEECH at 0x107da09d0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lUkVRKZtz7X"
      },
      "source": [
        "# Download LibriSpeech (train-clean-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJfq3sZ7s2qf",
        "outputId": "0b1e6701-4fb0-4354-b5d7-e0bfa43aa8df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample rate: 16000\n",
            "Transcript: CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK\n",
            "Waveform shape: torch.Size([1, 225360])\n"
          ]
        }
      ],
      "source": [
        "dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "\n",
        "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = dataset[0]\n",
        "\n",
        "print(f\"Sample rate: {sample_rate}\")\n",
        "print(f\"Transcript: {transcript}\")\n",
        "print(f\"Waveform shape: {waveform.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0K1Cud8t-0F"
      },
      "source": [
        "# Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY5kvd5qs44c"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import json\n",
        "\n",
        "def clean_data(min_duration=1.5, max_peak=0.95, sr=16000, save_dir=\"spectrogram_data\"):\n",
        "    dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "    # mel_transform = T.MelSpectrogram(sample_rate=sr, n_fft=512, hop_length=128, n_mels=64)\n",
        "\n",
        "    cleaned = []\n",
        "    \n",
        "    if os.path.exists('meta/clean_metadata.json'):\n",
        "        with open('meta/clean_metadata.json', 'r') as f:\n",
        "            return json.load(f)\n",
        "    \n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(\"meta\", exist_ok=True)\n",
        "\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        waveform, sample_rate, _, speaker_id, _, utt_id = dataset[i]\n",
        "\n",
        "        duration = waveform.shape[1] / sample_rate\n",
        "        peak_val = waveform.abs().max().item()\n",
        "\n",
        "        if duration >= min_duration and peak_val <= max_peak:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "            # Convert to spectrogram\n",
        "            # spec = mel_transform(waveform)\n",
        "\n",
        "            # filename = f\"{speaker_id}_{utt_id}.pt\"\n",
        "            # path = os.path.join(save_dir, filename)\n",
        "            # torch.save(spec, path)\n",
        "\n",
        "            cleaned.append({\n",
        "                \"index\": i,\n",
        "                \"duration\": duration,\n",
        "                \"speaker_id\": int(speaker_id),\n",
        "                \"utterance_id\": int(utt_id)\n",
        "                # \"spectrogram_path\": path\n",
        "            })\n",
        "\n",
        "\n",
        "    with open(\"meta/clean_metadata.json\", \"w\") as f:\n",
        "        json.dump(cleaned, f, indent=2)\n",
        "    \n",
        "    # print(f\"✅ Converted and saved {len(cleaned)} spectrograms to '{save_dir}'\")\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIY_qhJ31wRW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "\n",
        "def create_train_data(configs, output_dir=\"train_data\", seed=42):\n",
        "    # Set random seeds so we can reproduce results\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Load cleaned metadata\n",
        "    with open(\"meta/clean_metadata.json\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # Load dataset (no download here since we already have it)\n",
        "    dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(configs[\"num_samples\"]):\n",
        "        if configs[\"type\"] == \"cocktail\":\n",
        "            # Pick multiple random speakers\n",
        "            speakers = random.sample(metadata, configs[\"num_speakers\"])\n",
        "            waves = []\n",
        "\n",
        "            # Figure out the shortest clip (so we can align them all)\n",
        "            shortest_len = None\n",
        "            for s in speakers:\n",
        "                wave, sr, *_ = dataset[s[\"index\"]]\n",
        "                wave = wave[:, :int(sr * configs[\"duration\"])]\n",
        "                if shortest_len is None or wave.shape[1] < shortest_len:\n",
        "                    shortest_len = wave.shape[1]\n",
        "\n",
        "            # Truncate everything to match the shortest, apply random volume\n",
        "            for s in speakers:\n",
        "                wave, sr, *_ = dataset[s[\"index\"]]\n",
        "                wave = wave[:, :shortest_len]\n",
        "                amp = random.uniform(0.01, 0.1)  # really soft background voices\n",
        "                waves.append(amp * wave)\n",
        "\n",
        "            # Sum up all the quiet speakers — one louder target is at index 0\n",
        "            mix = sum(waves)\n",
        "            target = waves[0]  # target speaker is just the first one we picked\n",
        "\n",
        "        elif configs[\"type\"] == \"twospeaker\":\n",
        "            # Just pick two people to mix\n",
        "            s1, s2 = random.sample(metadata, 2)\n",
        "            wave1, sr, *_ = dataset[s1[\"index\"]]\n",
        "            wave2, sr, *_ = dataset[s2[\"index\"]]\n",
        "\n",
        "            # Truncate to match lengths\n",
        "            min_len = min(wave1.shape[1], wave2.shape[1])\n",
        "            wave1 = wave1[:, :min_len]\n",
        "            wave2 = wave2[:, :min_len]\n",
        "\n",
        "            # Speaker 1 is loud, speaker 2 is background\n",
        "            mix = 0.8 * wave1 + 0.2 * wave2\n",
        "            target = wave1\n",
        "\n",
        "        sf.write(f\"{output_dir}/mix_{i}.wav\", mix.squeeze().numpy(), sr)\n",
        "        sf.write(f\"{output_dir}/target_{i}.wav\", target.squeeze().numpy(), sr)\n",
        "\n",
        "    print(f\"✔️ Done! Made {configs['num_samples']} samples in '{output_dir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_data(num_samples, duration=5.0, seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Load dataset (no download here since we already have it)\n",
        "    dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "    with open(\"meta/clean_metadata.json\") as f:\n",
        "        metadata = json.load(f)\n",
        "    \n",
        "    mel_transform = T.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=1024,\n",
        "        hop_length=256,\n",
        "        n_mels=64\n",
        "    )\n",
        "    \n",
        "    examples = []\n",
        "    for _ in range(num_samples):\n",
        "        while True:\n",
        "            s1, s2 = random.sample(range(len(metadata)), 2)\n",
        "            meta1, meta2 = metadata[s1], metadata[s2]\n",
        "            if meta1[\"speaker_id\"] != meta2[\"speaker_id\"]:\n",
        "                break\n",
        "    \n",
        "        wave1, sr, *_ = dataset[meta1[\"index\"]]\n",
        "        wave2, sr, *_ = dataset[meta2[\"index\"]]\n",
        "\n",
        "        max_len = int(duration * sr)\n",
        "        wave1 = wave1[:, :max_len]\n",
        "        wave2 = wave2[:, :max_len]\n",
        "\n",
        "        # Pad if needed (ensure equal length)\n",
        "        def pad(wave, target_len):\n",
        "            if wave.shape[1] < target_len:\n",
        "                pad_amt = target_len - wave.shape[1]\n",
        "                wave = torch.nn.functional.pad(wave, (0, pad_amt))\n",
        "            return wave\n",
        "\n",
        "        wave1 = pad(wave1, max_len)\n",
        "        wave2 = pad(wave2, max_len)\n",
        "\n",
        "        # Mix the two: wave1 is target, wave2 is background\n",
        "        mix = 0.8 * wave1 + 0.2 * wave2\n",
        "\n",
        "        mix_spec = mel_transform(mix).squeeze(0)      # shape: [n_mels, T']\n",
        "        target_spec = mel_transform(wave1).squeeze(0)  # shape: [n_mels, T']\n",
        "\n",
        "        examples.append((mix_spec, target_spec))\n",
        "        \n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "b16EuK9tLY1U"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class SplitterRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "    super(SplitterRNN, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size=input_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        num_layers=num_layers,\n",
        "                        batch_first=True,\n",
        "                        dropout=dropout)\n",
        "    self.linear = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.transpose(1, 2)\n",
        "    output, _ = self.lstm(x)  # output: (batch_size, time_steps, hidden_size)\n",
        "    output = self.linear(output)  # output: (batch_size, time_steps, input_size)\n",
        "    return output.transpose(1,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGbCgeG5Tpj_",
        "outputId": "623af01a-ee26-4fda-df6a-5096f09bdc42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch mix shape: torch.Size([8, 1, 226640])\n",
            "Batch target shape: torch.Size([8, 1, 226640])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchaudio\n",
        "import os\n",
        "\n",
        "class SpeechSeparationDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        files = os.listdir(self.data_dir)\n",
        "        wav_files = [f for f in files if f.endswith(\".wav\")]\n",
        "        ids = {f.split(\"_\")[1].split(\".\")[0] for f in wav_files}\n",
        "        self.sample_ids = sorted(ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.sample_ids[idx]\n",
        "        mix_path = os.path.join(self.data_dir, f\"mix_{sample_id}.wav\")\n",
        "        target_path = os.path.join(self.data_dir, f\"target_{sample_id}.wav\")\n",
        "\n",
        "        mix, _ = torchaudio.load(mix_path)\n",
        "        target, _ = torchaudio.load(target_path)\n",
        "\n",
        "        return mix, target\n",
        "\n",
        "# TODO: Computational Experiment (pad or truncate??)\n",
        "def pad_inputs(batch):\n",
        "    mixed_audio, target_audio = zip(*batch)\n",
        "    mixed_audio = [x.squeeze(0) for x in mixed_audio]\n",
        "    target_audio = [x.squeeze(0) for x in target_audio]\n",
        "\n",
        "    mixes = pad_sequence(mixed_audio, batch_first=True)\n",
        "    targets = pad_sequence(target_audio, batch_first=True)\n",
        "\n",
        "    return mixes.unsqueeze(1), targets.unsqueeze(1)\n",
        "\n",
        "train_dataset = SpeechSeparationDataset(\"train_data\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pad_inputs)\n",
        "\n",
        "# Iterate through batches\n",
        "for mix_batch, target_batch in train_loader:\n",
        "    print(\"Batch mix shape:\", mix_batch.shape)\n",
        "    print(\"Batch target shape:\", target_batch.shape)\n",
        "    break\n",
        "\n",
        "# cocktail_config = {\n",
        "#     \"type\": \"cocktail\",\n",
        "#     \"num_samples\": 100,\n",
        "#     \"num_speakers\": 15,\n",
        "#     \"duration\": 3.0\n",
        "# }\n",
        "\n",
        "# twospeaker_config = {\n",
        "#     \"type\": \"twospeaker\",\n",
        "#     \"num_samples\": 5000,\n",
        "# }\n",
        "# clean_data()\n",
        "# create_train_data(cocktail_config)\n",
        "# create_train_data(twospeaker_config, output_dir='twospeaker_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25932\n"
          ]
        }
      ],
      "source": [
        "with open(\"meta/clean_metadata.json\") as f:\n",
        "    metadata = json.load(f)\n",
        "print(len(metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        self.examples = examples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "def collate_mel(batch):\n",
        "    mix_specs, target_specs = zip(*batch)  # list of [n_mels, T_i]\n",
        "\n",
        "    mix_padded = pad_sequence(mix_specs, batch_first=True)       # [B, T_max, n_mels]\n",
        "    target_padded = pad_sequence(target_specs, batch_first=True) # [B, T_max, n_mels]\n",
        "\n",
        "    # transpose to [B, n_mels, T] if your model expects that\n",
        "    mix_padded = mix_padded.transpose(1, 2)       # [B, n_mels, T]\n",
        "    target_padded = target_padded.transpose(1, 2) # [B, n_mels, T]\n",
        "\n",
        "    return mix_padded, target_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylQfmpik50Ed",
        "outputId": "43a34d01-4e56-40cd-9033-be61a2270ba4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "import torchaudio\n",
        "import os\n",
        "\n",
        "\n",
        "class Spectrogram(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        self.examples = examples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "# def collate_mel(batch):\n",
        "#     mix_specs, target_specs = zip(*batch)  # list of [n_mels, T_i]\n",
        "\n",
        "#     mix_padded = pad_sequence(mix_specs, batch_first=True)       # [B, T_max, n_mels]\n",
        "#     target_padded = pad_sequence(target_specs, batch_first=True) # [B, T_max, n_mels]\n",
        "\n",
        "#     # transpose to [B, n_mels, T] if your model expects that\n",
        "#     mix_padded = mix_padded.transpose(1, 2)       # [B, n_mels, T]\n",
        "#     target_padded = target_padded.transpose(1, 2) # [B, n_mels, T]\n",
        "\n",
        "#     return mix_padded, target_padded\n",
        "    \n",
        "def simple_collate(batch):\n",
        "    mix_specs, target_specs = zip(*batch)\n",
        "    mix_batch = torch.stack(mix_specs)       # [B, n_mels, T]\n",
        "    target_batch = torch.stack(target_specs) # [B, n_mels, T]\n",
        "    return mix_batch, target_batch\n",
        "\n",
        "valid_indices = clean_data(min_duration=5.0)\n",
        "constructed_dataset = create_data(valid_indices, 10000, 5.0)\n",
        "split = int(0.8 * len(constructed_dataset))\n",
        "train_examples = constructed_dataset[:split]\n",
        "test_examples = constructed_dataset[split:]\n",
        "\n",
        "train_dataset = SpectrogramDataset(train_examples)\n",
        "test_dataset = SpectrogramDataset(test_examples)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=simple_collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=simple_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "input_size = 64\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "batch_size = 8\n",
        "sequence_length = 100\n",
        "dropout = 0.25\n",
        "lr = 0.001\n",
        "\n",
        "model = SplitterRNN(input_size, hidden_size, num_layers, dropout)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "T4SvcNkzNk-U"
      },
      "outputs": [],
      "source": [
        "def train_rnn(model, criterion, optimizer, train_loader, nepochs=5000):\n",
        "    losses = []\n",
        "    model.train()\n",
        "    for e in range(nepochs):\n",
        "      total_loss = 0.0\n",
        "      for mixes_batch, tgt_batch in train_loader:\n",
        "          optimizer.zero_grad()          \n",
        "          output = model(mixes_batch)\n",
        "          loss = criterion(output, tgt_batch)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      losses.append(avg_loss)\n",
        "      if e % 5 == 0:\n",
        "        print(f\"Epoch {e:2d} | Loss: {avg_loss:.6f}\")\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_rnn(model, criterion, optimizer, train_loader, nepochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzHq_vKjNmBf"
      },
      "outputs": [],
      "source": [
        "def test_rnn():\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO8-9FPJJZXT"
      },
      "source": [
        "# Designing Computational Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNgtAhTOK8xv"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dziZFKyfM2wB"
      },
      "outputs": [],
      "source": [
        "pip install scipy sounddevice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "pj8qqM6sK8SH",
        "outputId": "7ac733df-0dbb-4426-ab33-549f261c49ab"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sounddevice'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7a97a6e9da78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# play_wav_file(\"path/to/your/audio.wav\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "\n",
        "# play_wav_file(\"path/to/your/audio.wav\")\n",
        "def play_audio(audio, sample_rate=44100):\n",
        "    \"\"\"\n",
        "    Plays an audio sample.\n",
        "\n",
        "    Parameters:\n",
        "    - audio (numpy.ndarray): The audio data. Can be mono (1D) or stereo (2D).\n",
        "    - sample_rate (int): Sampling rate in Hz (default: 44100).\n",
        "    \"\"\"\n",
        "    if not isinstance(audio, np.ndarray):\n",
        "        raise ValueError(\"Audio must be a NumPy array.\")\n",
        "\n",
        "    if audio.ndim > 2 or (audio.ndim == 2 and audio.shape[1] > 2):\n",
        "        raise ValueError(\"Audio must be mono or stereo.\")\n",
        "\n",
        "    sd.play(audio, samplerate=sample_rate)\n",
        "    sd.wait()  # Wait until audio playback is finished\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# visualize_wav(\"path/to/your/audio.wav\")\n",
        "def visualize_wav(filepath, show_spectrogram=True):\n",
        "    \"\"\"\n",
        "    Visualizes the waveform (and optionally spectrogram) of a WAV file.\n",
        "\n",
        "    Parameters:\n",
        "    - filepath (str): Path to the WAV file.\n",
        "    - show_spectrogram (bool): Whether to show the spectrogram below the waveform.\n",
        "    \"\"\"\n",
        "    # Load audio\n",
        "    data, samplerate = sf.read(filepath)\n",
        "    duration = len(data) / samplerate\n",
        "    time = np.linspace(0, duration, num=len(data))\n",
        "\n",
        "    # Set up plot\n",
        "    fig, axs = plt.subplots(2 if show_spectrogram else 1, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "    if not isinstance(axs, np.ndarray):\n",
        "        axs = [axs]  # ensure axs is always a list for consistency\n",
        "\n",
        "    # Plot waveform\n",
        "    axs[0].plot(time, data)\n",
        "    axs[0].set_title('Waveform')\n",
        "    axs[0].set_ylabel('Amplitude')\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Optional: Plot spectrogram\n",
        "    if show_spectrogram:\n",
        "        axs[1].specgram(data[:, 0] if data.ndim > 1 else data, Fs=samplerate, NFFT=1024, noverlap=512)\n",
        "        axs[1].set_title('Spectrogram')\n",
        "        axs[1].set_ylabel('Frequency [Hz]')\n",
        "        axs[1].set_xlabel('Time [s]')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Example:\n",
        "\n",
        "# Load audio files\n",
        "speaker1, sr = librosa.load(\"speaker1.wav\", sr=None)\n",
        "speaker2, _ = librosa.load(\"speaker2.wav\", sr=None)\n",
        "\n",
        "# Mix with speaker2 at 4x the volume of speaker1\n",
        "mixed, s1_adj, s2_adj = adjust_loudness_mix(speaker1, speaker2, loudness_ratio=4.0)\n",
        "\n",
        "# Save to disk\n",
        "librosa.output.write_wav(\"mixed.wav\", mixed, sr)\n",
        "\"\"\"\n",
        "def adjust_loudness_mix(speaker1_wave, speaker2_wave, loudness_ratio):\n",
        "    \"\"\"\n",
        "    Adjusts the loudness of speaker2 relative to speaker1.\n",
        "\n",
        "    Parameters:\n",
        "        speaker1_wave (np.ndarray): Audio waveform of speaker 1.\n",
        "        speaker2_wave (np.ndarray): Audio waveform of speaker 2.\n",
        "        loudness_ratio (float): Factor to scale speaker2's amplitude.\n",
        "                                 e.g., 2.0 means speaker2 is twice as loud as speaker1.\n",
        "\n",
        "    Returns:\n",
        "        mixed_wave (np.ndarray): The resulting mixture of the two speakers.\n",
        "        adjusted_speaker2 (np.ndarray): The scaled version of speaker2_wave.\n",
        "    \"\"\"\n",
        "    # Match length if needed\n",
        "    min_len = min(len(speaker1_wave), len(speaker2_wave))\n",
        "    speaker1_wave = speaker1_wave[:min_len]\n",
        "    speaker2_wave = speaker2_wave[:min_len]\n",
        "\n",
        "    # Apply loudness adjustment to speaker2\n",
        "    adjusted_speaker2 = speaker2_wave * loudness_ratio\n",
        "\n",
        "    # Mix the two signals\n",
        "    mixed_wave = speaker1_wave + adjusted_speaker2\n",
        "\n",
        "    # Optional: Normalize to avoid clipping\n",
        "    max_val = np.max(np.abs(mixed_wave))\n",
        "    if max_val > 1.0:\n",
        "        mixed_wave = mixed_wave / max_val\n",
        "        adjusted_speaker2 = adjusted_speaker2 / max_val\n",
        "        speaker1_wave = speaker1_wave / max_val\n",
        "\n",
        "    return mixed_wave, speaker1_wave, adjusted_speaker2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUpxVuhtCocL"
      },
      "source": [
        "# Nate does some delicious testing here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "VKfAA4XLDh7A",
        "outputId": "c228467d-09ae-43c7-faea-184480b3266b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'play_audio' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b81ebf930c8b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutterance_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'play_audio' is not defined"
          ]
        }
      ],
      "source": [
        "# dataset = torchaudio.datasets.LIBRISPEECH(root=\"./data\", url=\"train-clean-100\", download=False)\n",
        "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = dataset[0]\n",
        "\n",
        "play_audio(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8V-Z5kFKtOL"
      },
      "source": [
        "**Experiment 1 - adjust loudness of target input and compare loss**\n",
        "\n",
        "Goal: Assess how robust the model is when one speaker is significantly louder.\n",
        "\n",
        "Design:\n",
        "\n",
        "For each mixed sample, systematically vary the amplitude of one speaker:\n",
        "\n",
        " - Ratios: 1:1 (baseline), 2:1, 4:1, 8:1\n",
        "\n",
        "Evaluate performance degradation or adaptation.\n",
        "\n",
        "Hypotheses:\n",
        "\n",
        "MSE may degrade linearly with imbalance.\n",
        "\n",
        "CTC might maintain higher robustness if sequences are still distinguishable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STJpcYsAJUNf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQtlGyf8K138"
      },
      "source": [
        "Experiment 2 - adjust loudness of background noise and compare loss (I think this may combine with experiments 1 and 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYgarPFfK2Sx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIZx4O-OK2w-"
      },
      "source": [
        "**Experiment 3 - compare efficiency on different languages**\n",
        "\n",
        "Goal: Evaluate performance when one or both speakers speak a non-English language.\n",
        "\n",
        "Design:\n",
        "\n",
        "Use mixtures of English-English, English-Other (e.g., English-Spanish), and Other-Other.\n",
        "\n",
        "Focus on tonal vs non-tonal languages for diversity.\n",
        "\n",
        "Optional:\n",
        "\n",
        "Augment training data with multilingual samples.\n",
        "\n",
        "Try language embeddings if you're using a deeper pipeline.\n",
        "\n",
        "Analysis:\n",
        "\n",
        "Does the model performance degrade with unfamiliar phonetic structures?\n",
        "\n",
        "Does it favor English content due to training bias?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTvWI7qYK3GL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAfLe9pfL6km"
      },
      "source": [
        "**Experiment 4 - add non-human background noise and compare loss**\n",
        "\n",
        "Goal: Evaluate robustness to noise interference (e.g., cafe noise, car engines, music).\n",
        "\n",
        "Design:\n",
        "\n",
        "Add non-human noise at varying SNRs (e.g., 30 dB, 20 dB, 10 dB).\n",
        "\n",
        "Use both stationary and non-stationary noise.\n",
        "\n",
        "Compare:\n",
        "\n",
        "Performance with and without noise-aware preprocessing.\n",
        "\n",
        "Try noise suppression front-ends or data augmentation with noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm9HcraeL63U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPZ68Ohr4hVB"
      },
      "source": [
        "**Extra Experiments You Could Add**\n",
        "\n",
        "Speaker permutation invariance: Does the model consistently output the same speaker when order varies?\n",
        "\n",
        "Speaker gender and pitch: Investigate separation accuracy across gender combinations or pitch similarity.\n",
        "\n",
        "Temporal length variation: Does it work on long conversations vs short utterances?\n",
        "\n",
        "Window size sensitivity: Test how performance changes with different input chunk sizes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "342venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
